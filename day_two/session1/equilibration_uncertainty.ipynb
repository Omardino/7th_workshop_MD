{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2, Session 1, MD workshop\n",
    "### Dr. Michael Shirts, CU Boulder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we understand the information we get out of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up some modules.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up some typical data from a simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = np.loadtxt('potential.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(potential)\n",
    "plt.xlabel(\"Frame of Simulation\")\n",
    "plt.ylabel(\"Potential Energy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What should I report as the average potential energy of this simulation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the uncertainty in this average? i.e. the standard error of  the mean? Does this seem reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporting statistics for of non-normal distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betaf = scipy.stats.beta(0.8,4)\n",
    "x = np.linspace(0,2)\n",
    "plt.plot(x,betaf.pdf(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 10000\n",
    "samples = betaf.rvs(num)\n",
    "plt.hist(samples,bins=50,density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause and do some calculations\n",
    "\n",
    "* What would you report as the mean and standard deviation of an observation from this distribution?\n",
    "* What might be a better way to report the behavior of this distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential Answers:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(samples)\n",
    "print(f\"mean = {mean:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(samples)\n",
    "print(f\"std = {std:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the 95% confidence interval of an obserbation this distribution using the formula?\n",
    "print(mean - 2*std)\n",
    "print(mean + 2*std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better description\n",
    "xlow = np.percentile(samples,2.5)\n",
    "xhigh = np.percentile(samples,97.5)\n",
    "print(f\" 2.5% percentile is : {xlow:.4f}\")\n",
    "print(f\"97.5% percentile is : {xhigh:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about the error in the mean of this distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution_of_means(num,nrepeats):\n",
    "    repeats = np.zeros(nrepeats)\n",
    "    for i in range(nrepeats):\n",
    "        repeats[i] = np.mean(betaf.rvs(num))\n",
    "    return repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(2,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(5,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(distribution_of_means(100,10000),bins=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the formula for standard deviation of error now works.\n",
    "print(mean - 2*std/np.sqrt(100))\n",
    "print(mean + 2*std/np.sqrt(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining independent points: the autocorrelation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: All of this below is only valid for a _stationary_ timeseries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas implements an autocorrelation function as a function of _lag_ ($\\tau$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 500\n",
    "panda_pot = pd.Series(potential[start:]-np.mean(potential[start:]))\n",
    "panda_pot.autocorr(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use the `autocorr` function to calculate the autocorrelation function, as a function of lag time $\\tau$.  \n",
    " * Can you use your estimate of the autocorrelation function to find the the lag time $\\tau$ at which the system becomes uncorrelated (i.e. the autocorrelation function) to show points spaced more than $\\tau$ are independent? \n",
    " * **Note:** We also assume that the distribution is stationary, so we need to just do this analysis after equilibration has occurred.\n",
    " * **Also note:** the formulas for autorcorrelatiob assume that the average if the timeseries is zero, so you should subtract off the average first.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 500\n",
    "stationary = pd.Series(potential[start:])-np.mean(potential[start:])\n",
    "n_autocorr = np.shape(stationary-1)[0]\n",
    "nlim = int(n_autocorr/2)\n",
    "acf = np.zeros(nlim)\n",
    "for i in range(nlim):\n",
    "    acf[i] = stationary.autocorr(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(acf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or use the `pandas` utlilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh = pd.plotting.autocorrelation_plot(panda_pot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** At what point does the ACF become essentially zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting ACF to an exponential to estimate correlation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential(x, r): #function f(x, r) = e^(r*x)\n",
    "    #return np.e ** (r * x)\n",
    "    return np.exp(r*x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function calculates tau by using an exponential fit.\n",
    "#To do this it uses scipy.optimize curve_fit\n",
    "#curve_fit will optimize any function to fit given data\n",
    "\n",
    "def tau_calc(ac_data, function = exponential): #takes in data and a python function\n",
    "    x_data = np.arange(len(ac_data))\n",
    "    pars, cov = curve_fit(f=function, xdata=x_data, ydata=ac_data, p0=[0], bounds=(-np.inf, np.inf))\n",
    "    #curve fit returns an np.array of optimally fit paramters(pars) and their coverience(cov)\n",
    "    #pars in this case will return an optimized value of k to fit the dataset\n",
    "    return -1/pars #tau = -1/k\n",
    "# scipy curve_fit documentation: https://towardsdatascience.com/basic-curve-fitting-of-scientific-data-with-python-9592244a2509"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = tau_calc(acf)[0]\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\tau$ is the time it takes to go from 1 to $1/e$. So we actually can show we want to go out $2\\tau$ (there's some theory that this far enough!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.arange(100)\n",
    "plt.plot(x_data,acf[0:100], label='autocorrelation') \n",
    "plt.plot(exponential(x_data, -1/tau), label = 'tau_calc estimation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Key point*: If the data fits an exponential well, we can treat it as uncorrelated samples if they samples are $2\\tau$ apart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 500\n",
    "mean = np.mean(potential[start:])\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsamples = (len(potential)-start)/(2*tau)\n",
    "print(\"Nsamples =\", nsamples)\n",
    "stderr_mean = np.std(potential[start:])/np.sqrt(nsamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we use instead of `nsamples` above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Mean = {mean}, sderr_mean = {stderr_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating thermophysical observables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are probably interested in is the average potential energy per molecule, not the total potential energy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the heat of vaporization $H_{vap}$ by:\n",
    "    \n",
    "\\begin{eqnarray}\n",
    "H_{vap} &=& H_{gas}-H_{liquid} \\\\\n",
    "        &=& U_{gas} + PV_{gas} - (U_{liquid} + PV_{liquid}) \\\\ \n",
    "        &=& \\left(U_{gas} - U_{liquid}\\right) - P \\left(V_{gas}-V_{liquid}\\right) \\\\ \n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kinetic energy of liquid and vapor is the same, then this is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "U_{pot,gas} - U_{pot,liquid} + P(V_{gas}- V_{liquid})\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume ideality of gas ($PV=nRT$), and zero internal energy (which is valid for rigid water, like TIP3P or SPC/E), then we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray}\n",
    "       &=& - U_{pot,liquid} + P\\left( \\frac{nRT}{P} - V_{liquid}\\right) \\\\\n",
    "        &=& - U_{pot,liquid} + nRT - PV_{liquid} \\\\\n",
    "\\mathrm{H_{Molar}} &=& -\\frac{U_{pot,liquid}}{N} + RT - \\frac{PV_{liquid}}{N}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set potential.dat we have been playing with was from a simulation of TIP3P water with 900 molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the $\\frac{PV_{liquid}}{N}$ term is almost zero away from the critical point, so we actually can ignore it in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.101)*(18.02)/(900*1000)  # 18.02 g/mol x 1 L*cm^3 / 1000 g  x 1 atm x 0.101  kJ / L*atm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are left with: $\\mathrm{H_{Molar}} = -\\frac{U_{pot,liquid}}{N}+RT$, where $U_{pot,liquid}$ is the *average* potential energy of the liquid state $\\langle U \\rangle$ we have been calculating with above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "What is the $H_{vap}$ of water at 300 K predicted by this simulation? What is the uncertainty in the estimate? How does it compare to the experimental $H_{vap}$ of water at 300 K, which is 40.7 kJ/mol?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated tool for equilibration detection and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scipy statistical tests to see if two parts of the distributions are within the uncertainties of each other.  \n",
    "\n",
    "**Note**: We have to use uncorrelated samples, or it will erroneously say that they are NOT within uncertainties of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher T-statistic means more difference between the data sets.\n",
    "\n",
    "A lower P-value indicates a low probability that the difference in means was by chance.\n",
    "\n",
    "P-value < 0.05 suggest they are two different data sets! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:  Use this code to estimate better than \"eyeballing\" what fraction is equibrated  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "cut = 500\n",
    "stop = 1000\n",
    "per_ind_sample = 1  # the number of indices between independent samples\n",
    "# compare \n",
    "scipy.stats.ttest_ind(potential[start:cut:per_ind_sample],potential[cut:stop:per_ind_sample],equal_var=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definitely not independent!  At what point might they be independent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some additional tools (`pymbar.timeseries` equilibration detection), for example that can automate this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymbar import timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[t0, g, Nindep] = timeseries.detect_equilibration(potential)\n",
    "# t0 is the initial point detected as starting the stationary point.\n",
    "# g is the estimate of the correlation time (approximately 2tau)\n",
    "# Neff_max is an estimate of the _effective_ number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Equilibration time =',t0)\n",
    "print('Correlation time = ',g)\n",
    "print('Number of independent samples =', Nindep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
